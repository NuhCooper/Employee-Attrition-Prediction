{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Employee Attrition Prediction Project\n",
        "This project aims to predict whether an employee is at risk of leaving the company based on their attributes. By predicting employee attrition, we can help management take proactive measures to reduce turnover, ensuring talent retention and improved company culture. We use a dataset from IBM HR Analytics, applying machine learning techniques to build a classification model that provides actionable insights for HR strategy.\n",
        "\n",
        "## Project Objectives\n",
        "1. Load and explore the dataset.\n",
        "2. Perform data cleaning and preprocessing.\n",
        "3. Build and evaluate a machine learning model.\n",
        "4. Visualise the results to understand key factors affecting attrition."
      ],
      "metadata": {
        "id": "K5nGO3htKeYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Importing Libraries\n",
        "We begin by importing necessary libraries for data manipulation, visualisation, and model building. Libraries such as pandas and Scikit-learn will help us handle data and train models, while Seaborn and Matplotlib will be used to create insightful visualisations."
      ],
      "metadata": {
        "id": "jxIv1NDqKr6J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ofyumsYXIX5U"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for data handling, visualization, and machine learning\n",
        "\n",
        "import pandas as pd  # For data handling and manipulation\n",
        "import numpy as np  # For numerical operations\n",
        "import matplotlib.pyplot as plt  # For data visualization\n",
        "import seaborn as sns  # For more attractive visualizations\n",
        "from sklearn.model_selection import train_test_split  # For splitting the dataset into training and test sets\n",
        "from sklearn.preprocessing import LabelEncoder  # For encoding categorical variables\n",
        "from sklearn.linear_model import LogisticRegression  # For building the classification model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve  # For model evaluation\n",
        "\n",
        "# Set some default styling options for plots\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Loading the Dataset\n",
        "In this step, we load the IBM HR Analytics Employee Attrition dataset into a pandas DataFrame. This dataset includes information such as job role, satisfaction level, salary, and other features that could influence an employee's decision to stay or leave the company. By understanding these attributes, we can predict employee attrition effectively."
      ],
      "metadata": {
        "id": "kIWzvH06Jzk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset into a DataFrame\n",
        "\n",
        "# Note: The dataset file must be uploaded first.\n",
        "df = pd.read_csv('/content/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n",
        "\n",
        "# Display the first five rows of the dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qMCup-9rJr13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Exploratory Data Analysis (EDA)\n",
        "To understand the dataset thoroughly, we begin by checking its structure, identifying any missing values, and analysing descriptive statistics of the numerical columns. We also visualise the distribution of attrition to understand the balance between employees who stayed and those who left."
      ],
      "metadata": {
        "id": "DONKnBYwJ2S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of the dataset\n",
        "print(\"Dataset Information:\\n\")\n",
        "df.info()\n",
        "\n",
        "# Check for any missing values in the dataset\n",
        "print(\"\\nMissing Values Count:\\n\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Display a statistical summary of the numerical columns\n",
        "print(\"\\nStatistical Summary:\\n\")\n",
        "print(df.describe())\n",
        "\n",
        "# Count of employees who left the company vs. stayed\n",
        "sns.countplot(x='Attrition', data=df)\n",
        "plt.title(\"Employee Attrition Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AQvfpjOXJum_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Data Cleaning and Preprocessing\n",
        "Before building a machine learning model, it is crucial to clean the dataset. We convert categorical variables into numerical values using Label Encoding, making the data suitable for machine learning algorithms. This step ensures our features are in a usable format for the model."
      ],
      "metadata": {
        "id": "qmTOXX9bJ_0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variables to numerical ones using LabelEncoder\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# List of categorical columns to encode\n",
        "categorical_cols = ['Attrition', 'BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']\n",
        "\n",
        "# Apply encoding to each column\n",
        "for col in categorical_cols:\n",
        "    df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "# Display the updated dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "IpfB-snkKCF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Feature Selection and Data Splitting\n",
        "In this step, we define the target variable (`Attrition`) and split the dataset into training and test sets. This allows us to train the model on one part of the data and test it on unseen data, ensuring the model generalises well and does not overfit. We also remove any non-informative columns like `Over18` to streamline the dataset."
      ],
      "metadata": {
        "id": "2OZqEqJMKEmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target variable 'Attrition' and the feature variables 'X'\n",
        "target = 'Attrition'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Split the dataset into 80% training and 20% test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training Set: {X_train.shape}, Test Set: {X_test.shape}\")\n",
        "\n",
        "# Drop the Over18 column since it is not informative\n",
        "X_train = X_train.drop(columns=['Over18'])\n",
        "X_test = X_test.drop(columns=['Over18'])\n",
        "\n",
        "# Identify columns with object (string) data types\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "print(f\"Categorical columns: {list(categorical_cols)}\")"
      ],
      "metadata": {
        "id": "LOHZlNtrKI1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: One-Hot Encoding and Scaling\n",
        "To prepare our categorical variables for modelling, we apply One-Hot Encoding to convert them into numerical values, followed by scaling the features using `StandardScaler`. Scaling is particularly important for models like Logistic Regression to ensure all features contribute equally and improve convergence."
      ],
      "metadata": {
        "id": "-Ay1d6xLKK4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encode the categorical columns in X_train and X_test\n",
        "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
        "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
        "\n",
        "# Ensure X_train and X_test have the same columns after encoding\n",
        "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# Display a summary to ensure encoding worked\n",
        "print(\"Shape of X_train after encoding:\", X_train_encoded.shape)\n",
        "print(\"Shape of X_test after encoding:\", X_test_encoded.shape)\n",
        "\n",
        "# Save the column names for later use (since scaling removes them)\n",
        "original_columns = X_train_encoded.columns\n",
        "\n",
        "# Step 6: Scaling the Features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and test sets\n",
        "X_train_encoded = scaler.fit_transform(X_train_encoded)\n",
        "X_test_encoded = scaler.transform(X_test_encoded)"
      ],
      "metadata": {
        "id": "56JTu6FAKOMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Building the Logistic Regression Model\n",
        "We will build a Logistic Regression model to predict employee attrition. Logistic Regression is a simple yet effective baseline model for binary classification tasks."
      ],
      "metadata": {
        "id": "nbhu-V27KQe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Building - Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression model with enhanced configuration\n",
        "model = LogisticRegression(max_iter=5000, solver='liblinear')\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Make predictions using the test set\n",
        "y_pred = model.predict(X_test_encoded)"
      ],
      "metadata": {
        "id": "dm29k1SyKTIm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Model Evaluation\n",
        "After training the model, we evaluate its performance using metrics such as accuracy, confusion matrix, and classification report. These metrics help us understand the balance between true positives, false positives, and overall precision and recall. This information provides an indication of how well our model can identify employees at risk of attrition."
      ],
      "metadata": {
        "id": "k6kPCBoYKVEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Zx6JNsIBKXqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Feature Importance\n",
        "After training the model, we analyse which features had the most impact on predicting employee attrition. Understanding feature importance helps us identify key drivers behind an employee's decision to leave, which provides valuable insights for HR to improve retention strategies."
      ],
      "metadata": {
        "id": "0LRXAmKCTDqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the importance of features using the model coefficients\n",
        "feature_importance = pd.Series(abs(model.coef_[0]), index=original_columns)\n",
        "feature_importance = feature_importance.sort_values(ascending=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=feature_importance.head(10), y=feature_importance.head(10).index)\n",
        "plt.title(\"Top 10 Important Features Affecting Employee Attrition\")\n",
        "plt.xlabel(\"Coefficient Value (Importance)\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-msSrrQuTWhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary and Conclusion\n",
        "\n",
        "In this project, we have built a machine learning model to predict employee attrition using the IBM HR Analytics dataset. We explored the dataset, cleaned it, encoded categorical variables, and built a logistic regression model to predict whether an employee might leave.\n",
        "\n",
        "Our model provided insights into the key factors that affect employee attrition, which can help HR departments make informed decisions to reduce turnover.\n",
        "\n",
        "### Future Work\n",
        "- **Model Improvement**: We could try different machine learning models like Decision Trees, Random Forest, or even ensemble methods to see if we can achieve better accuracy.\n",
        "- **Feature Engineering**: We could create new features to provide deeper insights and improve model accuracy.\n",
        "- **Deployment**: The final model could be deployed as a web service to provide real-time predictions for employee attrition.\n"
      ],
      "metadata": {
        "id": "lM5HvAX9LpKd"
      }
    }
  ]
}